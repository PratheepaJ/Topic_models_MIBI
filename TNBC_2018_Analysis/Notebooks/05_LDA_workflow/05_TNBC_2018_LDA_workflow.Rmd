---
title: "05_TNBC_2018_LDA_workflow"
knit: (function(input, encoding) {
  rmarkdown::render(input = input,
                    output_dir = here::here("Output", "HTML"),
                    knit_root_dir = rprojroot::find_rstudio_root_file())})
output:
  html_document: 
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document: 
    toc: yes
date: "2023-01-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = "../")
#knitr::opts_knit$set(output.dir = "../Output_Documents/")
options(dplyr.summarise.inform = FALSE)
```


```{r warning=FALSE, message=FALSE}
#library(phyloseq)
library(dplyr)
library(magrittr)
library(ggplot2)
library(stringr)
#library(genefilter) 
library(rstan)
library(randomcoloR)
library(SpatialExperiment)
#library(DESeq2)
```

```{r}
theme_set(theme_minimal())
theme_update(
  text = element_text(size = 10),
  legend.text = element_text(size = 10)
)

# force not to use scientific notation
options(scipen=999)
```

Here, we are interested infer on the topics (bacterial communities) when we use original and modified PNA clams (two different experimental conditions). We use latent Dirichlet allocation to identify topics.

## Dataset

Load the TNBC SPE object.
```{r warning=FALSE, message=FALSE}
load(here::here("Output", "Data", "03_SPE", "03_TNBC_2018_spe.rds"))
spe
```


## Sample Data

We want to first apply the Latent Dirichlet Allocation to the sample data with patient 1 and patient 2 and find topics in their celluar neighbourhood.
```{r}
spe_sample <- spe[, spe$sample_id %in% c("Sample_01", "Sample_02")]
spe_sample
```

### Document-Term Matrix

We then want to create the Document-Term matrix with the sample dataset, where the rows are the documents (sample) and the columns are the words(cell-type) and filled with counts.
```{r}
# create document-term matrix
#dtm_sample <- as.data.frame.matrix(table(spe_sample$sample_id, spe_sample$mm)) %>% as.matrix()
#dtm_sample <- as.matrix(table(spe_sample$sample_id, spe_sample$mm))
dtm_sample <- table(spe_sample$sample_id, spe_sample$mm)

# dimension of document-term matrix
dim(dtm_sample)
```


We next to acquire the column names of the sample Document-Term matrix for further usage, and we set the colnames of the matrix to `NULL` as no names should be apply to the LDA function.
```{r}
# column names of document-term matrix
col_names_sample <- colnames(dtm_sample)
col_names_sample

# setting column names to NULL
colnames(dtm_sample) <- NULL
```


### Colour
Choose colors for plots. For the whole data, we can use nine colors for nine different plants in the dataset.

```{r}
plant_colors <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "purple")
```


### Input sample data for LDA

Based on the ordination, we choose the number of topics as K=3. We set the hyperparameters $\alpha = 0.8$ and $\gamma = 0.8$. 

```{r}
K <- 3
alpha <- 0.8
gamma <- 0.8
x <- dtm_sample
# View(x)
dimnames(x) <- NULL

# theta[d] ~ dirichlet(alpha), alpha pseudo-count for each topic
# beta[k] ~ dirichlet(gamma), gamma pseudo-count for each ASV in each topic

# n is DTM
stan.data <- list(K = K, 
  V = ncol(x), 
  D = nrow(x), 
  n = x, 
  alpha = rep(alpha, K), 
  gamma = rep(gamma, ncol(x))
)
```


### Posterior sampling 

We want to estimate the parameters $\theta$ and $B = \left[\beta_{1}, \cdots, \beta_{K} \right]^{T}$, and we estimate using Hamiltonian Monte Carlo(HMC) No-U-Turn-Sampler(NUTS) with one chain and 500 iterations. Out of these 500 iterations, 250 iteration were used as warm-up samples. We uses high-performance computing and save the results with specified file name.

We first want to specify a file name to save the results.
```{r}
# number of iteration
iter <- 500
# file name
fileN <- paste0(
  "JABES_Endo_all_K_",
  K,
  "_ite_",
  iter,
  "_sample",
  ".RData"
  )
```

By setting the number of chains to one, we apply with the sample dataset.
```{r posterior-sampling, eval = FALSE}
#set.seed(4547834)
set.seed(19991116)


# number of chains
chains <- 1

# apply dataset
t1 <- proc.time()
stan.fit <- stan(
  file = here::here("Notebooks", "06_LDA_scripts", "06_lda.stan"),
  data = stan.data, # must be list object
  iter = iter, 
  chains = 1, 
  sample_file = NULL,
  diagnostic_file = NULL,
  cores = 4,
  control = list(adapt_delta = 0.9),
  save_dso = TRUE,
  algorithm = "NUTS")
proc.time() - t1

save(stan.fit, 
     file = here::here("Output", "Data", "05_LDA_workflow", paste0(fileN)))
```

We then load the file from the directory and perform further analysis.
```{r eval=TRUE, include=FALSE}
load(file = here::here("Output", "Data", "05_LDA_workflow", paste0(fileN)))
```


We extract posterior samples from the results.
```{r extract-ps-sample}
sample_result <- rstan::extract(
  stan.fit, 
  permuted = TRUE, 
  inc_warmup = FALSE, 
  include = TRUE
  )
```


### Topics analysis

We would like to analyze the results from applying LDA to the sample dataset. Since we are interested in the parameters $\theta$ and $B = \left[\beta_{1}, \cdots, \beta_{K} \right]^{T}$, we want to extract them from the results and check their dimensions. 


#### $\theta$

We start with $\theta$. The dimension of $\theta$ is 250 x 2 x 3, where 250 is the number of iteration with 2 patients and 3 topics. Each value is the probability of each patient in each topic in each iteration.
```{r}
theta_sample <- sample_result$theta
dim(theta_sample)
```

We first want to extract patient 1, and then find the median and mean of probabilities in each topic out fo the 250 runs. From both results, we can easily see both methods agree patient 1 is $99/%$ topic 3 and patient 2 is $99/%$ topic 1.
```{r}
# Patient 1
#theta_sample[, 1, ]
print("--- Patient 1 ---")
print("----- Median Probabilities -----")
apply(data.frame(theta_sample[, 1, ]), 2, median)

print("----- Mean Probabilities -----")
apply(data.frame(theta_sample[, 1, ]), 2, mean)
```


```{r}
# Patient 2
#theta_sample[, 2, ]
print("--- Patient 2 ---")
print("----- Median Probabilities -----")
apply(data.frame(theta_sample[, 2, ]), 2, median)

print("----- Mean Probabilities -----")
apply(data.frame(theta_sample[, 2, ]), 2, mean)
```

We can also plot the probability distribution for each topic in patient 1.
```{r}
# topic 1
hist(theta_sample[, 1, 1], xlim = c(0,1),
     main = "Histogram of Probabilty of Topic 1")

# topic 2
hist(theta_sample[, 1, 2], xlim = c(0,1),
     main = "Histogram of Probabilty of Topic 2")

# topic 3
hist(theta_sample[, 1, 3], xlim = c(0,1),
     main = "Histogram of Probabilty of Topic 3")
```

#### $\beta$

We are now interest what cell types are in each topics, thus we want to preform analysis with $\beta$ estimation. Each value is the probability of each cell type in each topic in each iterations.
```{r}
# extract beta estimation
beta_sample <- sample_result$beta

# set dimension names for cell type column
dimnames(beta_sample)[3] <- list(col_names_sample)
```


We want to extract patient 1, and then find the median and mean of the probability of each cell type in each topics.

```{r}
print("--- Topic 1---")
print("----- Mean -----")
apply(data.frame(beta_sample[, 1, ]), 2, mean) %>% sort(decreasing = TRUE)
print("----- Median -----")
apply(data.frame(beta_sample[, 1, ]), 2, median) %>% sort(decreasing = TRUE)
```
```{r}
print("--- Topic 2---")
print("----- Mean -----")
apply(data.frame(beta_sample[, 2, ]), 2, mean) %>% sort(decreasing = TRUE)
print("----- Median -----")
apply(data.frame(beta_sample[, 2, ]), 2, median) %>% sort(decreasing = TRUE)
```

```{r}
print("--- Topic 3---")
print("----- Mean -----")
apply(data.frame(beta_sample[, 3, ]), 2, mean) %>% sort(decreasing = TRUE)
print("----- Median -----")
apply(data.frame(beta_sample[, 3, ]), 2, median) %>% sort(decreasing = TRUE)
```

From the above results, we can clearly see both methods agree that Mesenchymal, Other, Other Immune and Mac dominate topic 1; both methods also agree that Other, B, Mesenchymal, Other Immune dominate topic 3; however, the two methods has disagreement in the order and proportion of cell types dominating topic 2, although their probabilities are close to each other.



## Application

We now want to apply the Latent Dirichlet Allocation to the complete dataset with 39 patients to find topics in their celluar neighbourhood.
```{r}
# Inspect complete dataset
spe
```

### Document-Term Matrix

We first want to create the document-term matrix of the dataset where the rows are the sample identity and columns are the cell types fill with counts. The expected dimension of the DTM is 39 rows by 16 columns.

```{r}
dtm <- table(spe$sample_id, spe$mm)
dim(dtm)
dim_names <- dimnames(dtm)
col_names <- colnames(dtm)
```


### Posterior Sampling

We want to estimate the parameters $\theta$ and $B = \left[\beta_{1}, \cdots, \beta_{K} \right]^{T}$, and we estimate using Hamiltonian Monte Carlo(HMC) No-U-Turn-Sampler(NUTS) with one chain and 500, 1000, and 2000 iterations. Out of these iterations, half of the iteration were used as warm-up samples. 

We want to use `lapply()` function and high-performance computing and save the results with specified file name.

```{r}
get_lda_result <- function(K, alpha, gamma, dtm, iter, chains) {
  # setting seed will align topics
  # not applying seed, topics will differ but similar probabilities
  set.seed(19991116)
  
  K <- K
  alpha <- alpha
  gamma <- gamma
  x <- dtm
  # View(x)
  dimnames(x) <- NULL
  
  # theta[d] ~ dirichlet(alpha), alpha pseudo-count for each topic
  # beta[k] ~ dirichlet(gamma), gamma pseudo-count for each ASV in each topic
  
  # n is DTM
  stan.data <- list(
    K = K,
    V = ncol(x),
    D = nrow(x),
    n = x,
    alpha = rep(alpha, K),
    gamma = rep(gamma, ncol(x))
  )
  
  #number of iteration
  iter <- iter
  # file name
  fileN <- paste0("JABES_Endo_all_K_",
                  K,
                  "_ite_",
                  iter,
                  ".RData")
  
  # number of chains
  chains <- chains
  
  # apply dataset
  t1 <- proc.time()
  stan.fit <- stan(
    file = here::here("Notebooks", "06_LDA_scripts", "06_lda.stan"),
    data = stan.data,
    # must be list object
    iter = iter,
    chains = chains,
    sample_file = NULL,
    diagnostic_file = NULL,
    cores = 4,
    control = list(adapt_delta = 0.9),
    save_dso = TRUE,
    algorithm = "NUTS"
  )
  proc_time <- proc.time() - t1   # processing time
  
  # save file with specified name
  save(stan.fit, 
       file = here::here("Output", "Data", "05_LDA_workflow", paste0(fileN)))
  
  res <- rstan::extract(
    stan.fit,
    permuted = TRUE,
    inc_warmup = FALSE,
    include = TRUE
  )
  
  return(res)
}
```

```{r}
load_lda_file <- function(file) {
  # load lda file as stan.fit
  #load(here::here("Output", "Data", "05_LDA_workflow", paste0(file)))
  load(file)
  
  # extract 
  res <- rstan::extract(
    stan.fit,
    permuted = TRUE,
    inc_warmup = FALSE,
    include = TRUE
  )
  
  return(res)
}
```

```{r eval=FALSE}
sample_500 <- get_lda_result(K = 3, alpha = 0.8, gamma = 0.8,
               dtm = dtm, iter = 500, chains = 1)
sample_1000 <- get_lda_result(K = 3, alpha = 0.8, gamma = 0.8,
               dtm = dtm, iter = 1000, chains = 1)
sample_2000 <- get_lda_result(K = 3, alpha = 0.8, gamma = 0.8,
               dtm = dtm, iter = 2000, chains = 1)
```


```{r eval=TRUE}
sample_500  <- load_lda_file(
  file = here::here("Output", "Data", 
                    "05_LDA_workflow", "JABES_Endo_all_K_3_ite_500.RData") 
  )
sample_1000 <- load_lda_file(
  file = here::here("Output", "Data", 
                    "05_LDA_workflow", "JABES_Endo_all_K_3_ite_1000.RData")
  )
sample_2000 <- load_lda_file(
  file = here::here("Output", "Data", 
                    "05_LDA_workflow", "JABES_Endo_all_K_3_ite_2000.RData")
)
```



### Topics analysis

We would like to analyze the results from applying LDA to the sample dataset. Since we are interested in the parameters $\theta$ and $B = \left[\beta_{1}, \cdots, \beta_{K} \right]^{T}$, we want to extract them from the results and check their dimensions. 


#### $\theta$

We start with $\theta$. The dimension of $\theta$ is k x 39 x 3, where k is the number of iteration with 39 patients and 3 topics. Each value is the probability of each patient in each topic in each iteration.

We want to extract estimated $\theta$ value from each iteration run for different iterations.
```{r}
theta_500 <- sample_500$theta
dim(theta_500)
theta_1000 <- sample_1000$theta
dim(theta_1000)
theta_2000 <- sample_2000$theta
dim(theta_2000)
```

We then want to set sample identity label and topic label to each iteration run, to visualize easier and extract useful information.
```{r}
topic_names <- list(paste0("Topic_", 1:3))
#sample_names <- list(paste0("Sample_", dim_names[1]))
sample_names <- dim_names[1] |> unlist()
dimnames(theta_500)[3] <- dimnames(theta_1000)[3] <- dimnames(theta_2000)[3] <- topic_names
dimnames(theta_500)[2] <- dimnames(theta_1000)[2] <- dimnames(theta_2000)[2] <- dim_names[1]
```


Since the results are similar from iteration 500, 1000, and 2000. We want to find the median of the topic probabilities of all sample.
```{r}
theta_topics <- sapply(1:39,
  FUN = function(k) {
    apply(data.frame(theta_2000[, k, ]), 2, median) # |> round(6)
  }
) |> t() |> as.data.frame()

rownames(theta_topics) <- sample_names
theta_topics$Topic <- colnames(theta_topics)[apply(theta_topics, 1, which.max)]
#knitr::kable(theta_topics, align = "c")
formattable::formattable(theta_topics, align = "c")
```



#### $\beta$

We are now interest what cell types are in each topics, thus we want to preform analysis with $\beta$ estimation. The expected dimension is k x 3 x 16, where $k$ is the number of iteration for 3 topics and 16 cell types. Each value is the probability of each cell type in each topic in each iterations.
```{r}
beta_500 <- sample_500$beta
dim(beta_500)
beta_1000 <- sample_1000$beta
dim(beta_1000)
beta_2000 <- sample_2000$beta
dim(beta_2000)
```


We want to set name of the cell types to the beta estimation to ensure the correctness of our conclusion.
```{r}
dimnames(beta_500)[3] <- dimnames(beta_1000)[3] <- dimnames(beta_2000)[3] <- list(col_names)
```


Since the estimation are similar between 500, 1000, and 2000 times of iteration, we want to use 2000 iterations estimated result. We want to find the median of the estimated results, and sort them with descending order for easier visulization.
```{r}
apply(data.frame(beta_500[, 1, ]), 2, mean) %>% sort(decreasing = TRUE)
print("------")
apply(data.frame(beta_1000[, 1, ]), 2, mean) %>% sort(decreasing = TRUE)
print("------")
apply(data.frame(beta_2000[, 1, ]), 2, mean) %>% sort(decreasing = TRUE)
```



```{r}
# topic 1
apply(data.frame(beta_2000[, 1, ]), 2, median) %>% sort(decreasing = TRUE)
```

The five cell types with the highest probability in topic 1 is Other, Mesenchymal, Neu, Epithelial, and Mono Neu.


```{r}
# topic 2
apply(data.frame(beta_2000[, 2, ]), 2, median) %>% sort(decreasing = TRUE)
```

The five cell types with the highest probability in topic 2 is Epithelial, Mesenchymal, CD8 T, Other Immune, and Mac.

```{r}
# topic 3
apply(data.frame(beta_2000[, 3, ]), 2, median) %>% sort(decreasing = TRUE)
```

The five cell types with the highest probability in topic 3 is B, CD4 T, CD8 T, Other Immune, and CD3 T.